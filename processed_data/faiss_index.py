# -*- coding: utf-8 -*-
"""FAISS_index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vC0p8-I1zOJvoKle8dPCV4teJBJYpPim
"""

pip install langchain_community

#양자화에 필요한 패키지 설치
pip install -q -U bitsandbytes
pip install -q -U git+https://github.com/huggingface/transformers.git
pip install -q -U git+https://github.com/huggingface/peft.git
pip install -q -U git+https://github.com/huggingface/accelerate.git
pip install faiss-cpu
pip install faiss-gpu
pip install sentence-transformers

"""# FIASS 인덱스 생성"""

import json
import faiss
from sentence_transformers import SentenceTransformer
import concurrent.futures
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# -------------------------
# Step 1: Data Loading
# -------------------------

file_paths = {
    "mct": '/content/mct.json',
    "month": '/content/month.json',
    "wkday": '/content/wkday.json',
    "mop_sentiment": '/content/merge_mop_sentiment.json',
    "menu": '/content/mct_menus.json',
    "visit_jeju": '/content/visit_jeju.json',
    "kakaomap_reviews": '/content/kakaomap_reviews.json'
}

data = {}
for key, path in file_paths.items():
    with open(path, 'r', encoding='utf-8') as f:
        data[key] = json.load(f)


# -------------------------
# Step 2: Embedding and FAISS Setup
# -------------------------

# SentenceTransformer 모델 설정
embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')

# FAISS PQ index 파라미터 설정
dimension = 768
m, nbits = 16, 8

# 데이터셋의 크기에 따라 nlist 설정
def get_nlist(data_size):
    return min(100, max(1, data_size // 10))

# 각 데이터셋에 맞는 인덱스 생성
indices_pq = {
    key: faiss.IndexIVFPQ(faiss.IndexFlatL2(dimension), dimension, get_nlist(len(data[key])), m, nbits)
    for key in data.keys()
}

import time
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import torch

# GPU 확인
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# SentenceTransformer 모델
embedding = SentenceTransformer('jhgan/ko-sroberta-multitask').to(device)

# 벡터 생성 함수 (GPU에서 병렬 처리 및 시간 추적)
def create_training_vectors(dataset, key, batch_size=32):
    texts = [item[key] for item in dataset]
    vectors = []
    start_time = time.time()  # 시작 시간 기록
    total_batches = len(texts) // batch_size + 1
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        # 배치 단위로 GPU에서 벡터 생성
        batch_vectors = embedding.encode(batch_texts, show_progress_bar=False, device=device)
        vectors.extend(batch_vectors)

        # 배치 처리 시간 추적
        batch_time = time.time() - start_time
        avg_batch_time = batch_time / (i // batch_size + 1)
        remaining_batches = total_batches - (i // batch_size + 1)
        estimated_time_remaining = avg_batch_time * remaining_batches

        print(f"Processed batch {i//batch_size + 1}/{total_batches} - Estimated time remaining: {estimated_time_remaining:.2f} seconds")

    total_time = time.time() - start_time
    print(f"Total time for vector creation: {total_time:.2f} seconds")
    return np.array(vectors).astype('float32')

def train_and_add_index(index_pq, dataset, key, filename, num_clusters=64):
    start_time = time.time()  # 시작 시간 기록
    vectors = create_training_vectors(dataset, key)

    # 데이터 포인트 수와 클러스터 수를 비교하여 클러스터 수 설정
    num_clusters = min(len(vectors), num_clusters)  # 데이터보다 많은 클러스터 수는 설정하지 않음

    if num_clusters <= 1:
raise ValueError("클러스터 수는 최소 2개 이상이어야 합니다.")

    # 인덱스 훈련 전에 클러스터 수 설정
    quantizer = faiss.IndexFlatL2(vectors.shape[1])  # 벡터 차원에 맞게 인덱스 생성
    index_pq = faiss.IndexIVFPQ(quantizer, vectors.shape[1], num_clusters, 8, 8)  # 8은 비트 수, 필요에 따라 조정

    try:
        index_pq.train(vectors)  # 훈련
        index_pq.add(vectors)    # 벡터 추가
        faiss.write_index(index_pq, filename)  # 인덱스 파일로 저장
        total_time = time.time() - start_time
        print(f"Time taken for index {filename}: {total_time:.2f} seconds")
    except Exception as e:
        print(f"Error during FAISS index creation: {e}")

# 각 데이터셋에 대해 FAISS 인덱스 생성 및 저장 (상태 출력)
train_and_add_index(indices_pq['mct'], data['mct'], '가게명', 'mct_index_pq.faiss')
print("mct 인덱스가 완료되었습니다.")

train_and_add_index(indices_pq['month'], data['month'], '관광지명', 'month_index_pq.faiss')
print("month 인덱스가 완료되었습니다.")

train_and_add_index(indices_pq['wkday'], data['wkday'], '관광지명', 'wkday_index_pq.faiss')
print("wkday 인덱스가 완료되었습니다.")

# 각 데이터셋에 대해 FAISS 인덱스 생성 및 저장 (상태 출력)
train_and_add_index(indices_pq['kakaomap_reviews'], data['kakaomap_reviews'], '관광지명', 'kakaomap_reviews.faiss')
print("kakaomap_reviews 인덱스가 완료되었습니다.")

# 각 데이터셋에 대해 FAISS 인덱스 생성 및 저장 (상태 출력)
train_and_add_index(indices_pq['visit_jeju'], data['visit_jeju'], '관광지명', 'visit_jeju.faiss')
print("visit_jeju 인덱스가 완료되었습니다.")

# 각 데이터셋에 대해 FAISS 인덱스 생성 및 저장 (상태 출력)
train_and_add_index(indices_pq['menu'], data['menu'], '가게명', 'menu.faiss')
print("menu 인덱스가 완료되었습니다.")

# -------------------------
# Step 4: Query Processing (Example)
# -------------------------

def query_pipeline(query_text):
    # Tokenize input query
    inputs = tokenizer(query_text, return_tensors="pt").to(model.device)

    # Generate response using Gemini model
    outputs = model.generate(**inputs)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Example FAISS search (search on mct index as a sample)
    embedding_vector = embedding.encode(query_text).astype('float32')
    _, top_indices = indices_pq['mct'].search(np.array([embedding_vector]), k=5)

    # Retrieve and print results
    print("Generated Response:", response)
    print("Top 5 FAISS Results:", top_indices)

# Example query
query_pipeline("Find a popular attraction in Jeju Island")

"""# FAISSDB 생성"""

# FAISS 라이브러리 설치
pip install faiss-gpu

import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

import json

# Define the paths for all your JSON files in a dictionary
file_paths = {
    "mct": '/content/mct.json',
    "month": '/content/month.json',
    "wkday": '/content/wkday.json',
    "mop_sentiment": '/content/merge_mop_sentiment.json',
    "menu": '/content/mct_menus.json',
    "visit_jeju": '/content/visit_jeju.json',
    "kakaomap_reviews": '/content/kakaomap_reviews.json'
}

# Initialize an empty dictionary to store the loaded data
data = {}

# Load each JSON file and store it in the dictionary
for key, path in file_paths.items():
    with open(path, 'r', encoding='utf-8') as file:
        data[key] = json.load(file)

from langchain.docstore.document import Document

# 리스트 항목에서 '가게명'을 사용하여 Document 객체 생성
mct_docs = [Document(page_content=item['가게명'], metadata=item) for item in data['mct']]
month_docs = [Document(page_content=item['관광지명'], metadata=item) for item in data['month']]
wkday_docs = [Document(page_content=item['관광지명'], metadata=item) for item in data['wkday']]
mop_docs = [Document(page_content=item['관광지명'], metadata=item) for item in data['mop_sentiment']]
menu_docs = [Document(page_content=item['가게명'], metadata=item) for item in data['menu']]
visit_docs = [Document(page_content=item['관광지명'], metadata=item) for item in data['visit_jeju']]
kakaomap_reviews_docs = [Document(page_content=item['관광지명'], metadata=item) for item in data['kakaomap_reviews']]

# -------------------------
# Step 3: Initialize jhgan/ko-sroberta-multitask Model
# -------------------------

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Model and tokenizer setup for jhgan/ko-sroberta-multitask
model_id = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")

print(model)

pip install sentence-transformers

from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
import json

# Initialize the SentenceTransformerEmbeddings
embedding = SentenceTransformerEmbeddings(model_name="jhgan/ko-sroberta-multitask")

# 각 문서 리스트를 FAISS DB에 넣기
mct_db = FAISS.from_documents(documents=mct_docs, embedding=embedding)
month_db = FAISS.from_documents(documents=month_docs, embedding=embedding)
wkday_db = FAISS.from_documents(documents=wkday_docs, embedding=embedding)
mop_db = FAISS.from_documents(documents=mop_docs, embedding=embedding)
menus_db = FAISS.from_documents(documents=menu_docs, embedding=embedding)
visit_db = FAISS.from_documents(documents=visit_docs, embedding=embedding)
kakaomap_reviews_db = FAISS.from_documents(documents=kakaomap_reviews_docs, embedding=embedding)

print(model)

pip -q install langchain pypdf chromadb sentence-transformers faiss-gpu

pip install langchain-community

"""### 벡터스토어 기반 검색기(VectorStore-backed Retriever)"""

# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당
mct_retriever = mct_db.as_retriever()
month_retriever = month_db.as_retriever()
wkday_retriever = wkday_db.as_retriever()
mop_retriever = mop_db.as_retriever()
mct_menus_retriever = menus_db.as_retriever()
visit_retriever = visit_db.as_retriever()
kakaomap_reviews_retriever = kakaomap_reviews_db.as_retriever()

# 관련 문서를 검색
docs = mct_retriever.invoke("제주도 공항 근처 카페?")

for doc in docs:
    print(doc.page_content)
    print("=========================================================")

"""### 앙상블 검색기(EnsembleRetriever)"""

from langchain.retrievers import EnsembleRetriever

from langchain.retrievers import BM25Retriever

# 각 DB에서 MMR 기반 검색기로 리트리버 초기화
mct_retriever = mct_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

month_retriever = month_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

wkday_retriever = wkday_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

mop_retriever = mop_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

mct_menus_retriever = menus_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

visit_retriever = visit_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

kakaomap_reviews_retriever = kakaomap_reviews_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.6, "score_threshold": 0.8}
)

pip install rank-bm25

# BM25 검색기 생성
mct_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in mct_docs])
mct_menus_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in mct_menus_docs])
mop_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in mop_docs])
month_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in month_docs])
visit_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in visit_docs])
wkday_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in wkday_docs])
kakaomap_reviews_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in kakaomap_reviews_docs])

import torch
from langchain.retrievers import EnsembleRetriever
from sentence_transformers import util

# 앙상블 검색기 초기화
mct_ensemble_retriever = EnsembleRetriever(
    retrievers=[mct_retriever, mct_bm25_retriever],
    weights=[0.6, 0.4]  # 필요에 따라 가중치 조정
)

mct_menus_ensemble_retriever = EnsembleRetriever(
    retrievers=[mct_menus_retriever, mct_menus_bm25_retriever],
    weights=[0.6, 0.4]
)

mop_ensemble_retriever = EnsembleRetriever(
    retrievers=[mop_retriever, mop_bm25_retriever],
    weights=[0.6, 0.4]
)

month_ensemble_retriever = EnsembleRetriever(
    retrievers=[month_retriever, month_bm25_retriever],
    weights=[0.6, 0.4]
)

visit_ensemble_retriever = EnsembleRetriever(
    retrievers=[visit_retriever, visit_bm25_retriever],
    weights=[0.6, 0.4]
)

wkday_ensemble_retriever = EnsembleRetriever(
    retrievers=[wkday_retriever, wkday_bm25_retriever],
    weights=[0.6, 0.4]
)

kakaomap_reviews_ensemble_retriever = EnsembleRetriever(
    retrievers=[kakaomap_reviews_retriever, kakaomap_reviews_bm25_retriever],
    weights=[0.6, 0.4]
)

# 유연한 펑션 콜링을 위한 검색 및 병합 함수
def flexible_function_call_search(query): # 사용자 입력(query)을 받아, 적합한 검색기들을 선택하고 검색 결과를 반환하는 함수
    input_embedding = embedding.embed_query(query) # 입력 쿼리를 임베딩

    retriever_mappings = {
        "mct": mct_ensemble_retriever,
        "mct_menus": mct_menus_ensemble_retriever,
        "mop": mop_ensemble_retriever,
        "month": month_ensemble_retriever,
        "visit": visit_ensemble_retriever,
        "wkday": wkday_ensemble_retriever,
        "kakaomap_reviews": kakaomap_reviews_ensemble_retriever
    } # 각 검색기의 이름과 실제 검색기 객체를 매핑

    retriever_descriptions = {
        "mct": "식당명 및 이용 비중 및 금액 비중",
        "mct_menus": "식당명 및 메뉴 및 금액",
        "mop": "관광지 전체 키워드 분석 데이터",
        "month": "관광지 월별 조회수",
        "visit": "관광지 핵심 키워드 및 정보",
        "wkday": "주별 일별 조회수 및 연령별 성별별 선호도",
        "kakaomap_reviews": "리뷰 데이터"
    } # 검색기와 관련된 설명을 포함한 딕셔너리로, 각 검색기의 목적을 설명

    retriever_embeddings = {key: embedding.embed_query(value) for key, value in retriever_descriptions.items()}
    similarities = {key: util.cos_sim(input_embedding, torch.tensor(embed)).item() for key, embed in retriever_embeddings.items()}

    selected_retrievers = [key for key, sim in similarities.items() if sim >= 0.5]
    if not selected_retrievers:
        selected_retrievers = [max(similarities, key=similarities.get)] # 유사도가 0.5 이상인 검색기들을 selected_retrievers 리스트에 추가합니다. 만약 기준을 만족하는 검색기가 없다면, 가장 높은 유사도를 가진 검색기를 선택

    combined_results = {}
    for retriever in selected_retrievers:
        search_result = retriever_mappings[retriever].invoke(query) # 선택된 검색기들에 대해 invoke() 메서드를 호출하여 사용자 쿼리를 전달하고 검색을 수행
        combined_results[retriever] = search_result # 각 검색기의 결과가 combined_results 딕셔너리에 저장

    # 결과 병합 및 응용 로직
    merged_results = []
    for key, docs in combined_results.items():
        for doc in docs:
            if doc.page_content not in [result.page_content for result in merged_results]:
                merged_results.append(doc)

    return merged_results # 최종 병합된 검색 결과 리스트를 반환

from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

from langchain.prompts import PromptTemplate, ChatPromptTemplate

prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
    ### 역할
    당신은 제주도 맛집과 관광지 추천 전문가입니다. 복잡한 질문일수록 논리적으로 분석한 후 단계별로 답변을 제공합니다.
    검색된 결과 외의 정보를 추가하지 마세요.
    주어진 데이터 외에 새로운 검색을 수행하지 마세요.

    ### Chain of Thought 방식:
    1. 질문의 핵심 내용을 분석합니다.
    2. 관련된 검색 결과를 기반으로 추천을 생성합니다.
    3. 답변을 사용자에게 체계적으로 제공합니다.

    검색된 결과:
    {search_results}

    대화 기록:
    {chat_history}

    사용자의 질문: {input_text}

    답변:
    """
)

"""# 언어모델 생성(Create LLM)"""

pip install google-ai-generativelanguage==0.6.10
pip install protobuf==5.28.3

from langchain_google_genai import ChatGoogleGenerativeAI

# Google Generative AI API 설정
llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash',
                                    api_key="AIzaSyBuyJrVL9GZmrhJXetAG3VMoziiYk1SNUQ",
                                    temperature=0.2,  # 더 낮은 temperature로 설정해 할루시네이션 줄임
                                    top_p=0.85,        # top_p를 조정해 더 예측 가능한 답변 생성
                                    frequency_penalty=0.1,  # 같은 단어의 반복을 줄이기 위해 패널티 추가
                                    stream=True  # 실시간 스트림 출력 활성화
)

"""## LangChain Expression Language(LCEL)"""

from langchain.chains import LLMChain

from langchain.chains import LLMChain

# 체인 생성
chain = LLMChain( # LangChain에서 제공하는 체인 객체로, LLM에 입력 데이터를 전달하고 출력을 반환하는 역할
    prompt=prompt_template,
    llm=llm,
    output_parser=StrOutputParser() # 생성된 응답을 파싱하는 객체로, StrOutputParser()를 사용해 응답을 문자열 형태로 반환
)

# 챗봇 대화 루프

def chat():
    print("챗봇 대화를 시작합니다. 'exit'을 입력하면 종료됩니다.")
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    while True:
        user_input = input("질문을 입력하세요: ")
        if user_input.lower() == "exit":
            break

        search_results = flexible_function_call_search(user_input)
        # 함수를 호출하여 사용자 입력을 기반으로 검색을 수행하고, 관련된 결과를 반환

        search_results_str = "\n".join([doc.page_content for doc in search_results])
        # 검색 결과는 문서 객체 리스트로 반환. 각 문서 객체의 page_content를 문자열로 변환하여 \n로 구분된 단일 문자열로 변환. 이 문자열은 LLM 체인에 전달되어 응답 생성을 위한 프롬프트에 포함.

        chat_history = memory.load_memory_variables({})["chat_history"]
        # memory.load_memory_variables()를 호출하여 이전 대화의 기록을 불러온다.

        input_data = {
            "input_text": user_input,
            "search_results": search_results_str,
            "chat_history": chat_history
        }
        # input_data 딕셔너리를 생성하여 LLM 체인에 전달할 데이터를 준비

        output = chain(input_data) # LLM 체인 호출
        output_text = output.get("text", str(output)) # input_data는 LLM 체인에 전달되어 프롬프트 템플릿에 삽입되고, LLM이 이를 기반으로 응답을 생성

        print("\n챗봇 응답:", output_text)
        memory.save_context({"input": user_input}, {"output": output_text})

# 대화 실행
chat()

